{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5deeb573-abd3-4819-bcfe-6c032179c03e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Projeto de extração de dados I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9864bbb3-40eb-46a1-bda5-2b2309644ec1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Resumo:** <br>\n",
    "* Campos: Genômica e medicina personalizada. <br>\n",
    "* Dados atualizados: (tendências, novidades e tratamentos) dessas áresas. <br>\n",
    "* Fazer um processo para receber os dados, carregar e tratar essas informações gerando insights valiosos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd7440cf-8746-4b5a-a3e6-c3b6fff47790",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Instalando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d22e55-1d06-4673-a6e9-1da2eff4bcab",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "603505c9-7d0e-4d3d-9bd9-fe06ad21213a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398fa4e6-ec2a-43c7-be13-efd0656a154d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests                     # Requisições de API.\n",
    "import pyspark.pandas as ps         # Trabalhar com dados estruturados.\n",
    "from pyspark.dbutils import DBUtils # Realizar operações ELT.\n",
    "import flask                        # Criar Webhook.\n",
    "from datetime import datetime       # Trabalhar com datas.\n",
    "import time as t                    # Trabalhar com tempos (Temporazidor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72cf48e4-b4c5-4e4f-9613-bbcdd69622cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Definições e variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34372d55-61bb-459d-abfd-4b1eadd97684",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar a Key da API.\n",
    "%run ./api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f0cbca3-ec29-413e-b13f-334544829a22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Diretórios para cada camada da Arquitetura Midellion.\n",
    "dir_base = '/FileStore/tables/Projetos'                     # Diretório base.\n",
    "dir_landing_zone = f'{dir_base}/landing_zone'              # Dados brutos da API.\n",
    "dir_bronze = f'{dir_base}/bronze/dado_consolidado.parquet' # Dados consolidados.\n",
    "dir_silver = f'{dir_base}/silver/dado_limpo.parquet'       # Dado consolidado e limpos.\n",
    "dir_gold = f'{dir_base}/gold'                              # Dados transformados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4842f82e-07cf-438e-bc59-28ece7008038",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a8bbc97-c1ad-4f22-bb37-f31a496c76cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Função para Extração dos dados (Landing zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e8fb13-cc1d-4cc1-8bcd-8a06a48250cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def consulta_api(current_datetime : str) -> dict:\n",
    "    \"\"\"\n",
    "    Consulta API e retorna os dados.\n",
    "    \n",
    "    Parâmetros:\n",
    "        current_datetime (str) : data para consulta.\n",
    "    \n",
    "    Retorno:\n",
    "        dados (dict): Dados da API.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parâmetros API.\n",
    "    by='popularity'\n",
    "\n",
    "    q_base = 'genome OR (personalized AND medicine)'\n",
    "    q_filtro1 = '(+DNA)'\n",
    "    q_filtro2 = '(+gene AND +therapies)'\n",
    "    q_filtro3 = '(+genetic AND +diseases)'\n",
    "    q_search = f'{q_base} AND ({q_filtro1} OR {q_filtro2} OR {q_filtro3})' \n",
    "\n",
    "    url = f'https://newsapi.org/v2/everything?q={q_search}&sortBy={by}&apiKey={API_KEY}&from={current_datetime}&to={current_datetime}'\n",
    "\n",
    "    # Buscar dados da API.\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200: # Caso consiga fazer a requisição.\n",
    "        dados = response.json()\n",
    "        \n",
    "        # Pegar os dados principais.\n",
    "        dados = dados['articles']\n",
    "        \n",
    "        if len(dados) > 0:\n",
    "            message = 'Aquisição de dados via API concluída'\n",
    "        else:\n",
    "            message = 'Sem dados de retorno via API'\n",
    "            dados = None\n",
    "\n",
    "    else: # Caso tenha um código diferente de 200.\n",
    "        dados = None\n",
    "        message = 'Aquisição de dados via API falhou, código de status: ' + str(response.status_code) \n",
    "\n",
    "    return dados, message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27dcb609-e5e5-43e8-97ee-92252d9c9fd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def melhora_dados_api(dados : dict) -> dict:\n",
    "    \"\"\"\n",
    "    Função que melhora os dados da API.\n",
    "    \n",
    "    Parâmetros:\n",
    "        dados (dict): Dados da API.\n",
    "        \n",
    "    Retorno:\n",
    "        dados (dict): Dados melhorados.\n",
    "    \"\"\"\n",
    "\n",
    "    # data da consulta.\n",
    "    data_atual = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Para cada item.\n",
    "    for i in range(len(dados)):\n",
    "        # Arrumando a coluna 'source'.\n",
    "        name = dados[i]['source']['name']\n",
    "        dados[i]['source'] = name\n",
    "        \n",
    "        # Adicionar data atual no dicionário.\n",
    "        dados[i]['data_load'] = data_atual\n",
    "\n",
    "        # Criar uma key para cada item.\n",
    "        key = dados[i]['url'] + dados[i]['publishedAt']\n",
    "\n",
    "        # Adicionar key.\n",
    "        dados[i]['key'] = key\n",
    "\n",
    "    return dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "978dbf9a-056a-488b-bf1b-7bd8c1f34101",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_data(current_datetime : str) -> ps.DataFrame:\n",
    "    \"\"\"\n",
    "    Função que extrai os dados da API e retorna um DataFrame, e salva um arquivo no diretório Landing_zone.\n",
    "    \n",
    "    Parâmetros:\n",
    "        current_datetime (str) : data para importar os dados.\n",
    "\n",
    "    Retorno:\n",
    "        df_result (ps.DataFrame): DataFrame com os dados extraídos da API.\n",
    "    \"\"\"\n",
    "\n",
    "    # Faz a consulta na API e retorna um dicionário.\n",
    "    dados, message = consulta_api(current_datetime)\n",
    "    \n",
    "    print(message) # Mostra a mensagem do retorno da API.\n",
    "    if dados:\n",
    "        # Melhorar dados e adiciona a Key.\n",
    "        dados = melhora_dados_api(dados)\n",
    "\n",
    "        # Transforma o dicionário em um DataFrame.\n",
    "        df_result = ps.DataFrame(dados) \n",
    "        \n",
    "        # Exportar os dados para um arquivo CSV na Landing Zone.\n",
    "        df_result.to_csv(f'{dir_landing_zone}/dados_extraidos.csv')\n",
    "\n",
    "        print('Resultado extraído com sucesso!')\n",
    "\n",
    "        return df_result\n",
    "    else:\n",
    "        print('Falha: Nenhum dado foi extraído da API!')\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27df6740-e46c-4aba-b7db-88e390e66a4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Função para Carregamento dos dados extraídos (Bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaae4763-2b22-4fdf-8ecc-30367f0e2e0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_data(df_new : ps.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Concatena os dados extraídos para o parquet consolidado.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_new (ps.DataFrame): DataFrame com os dados extraídos da API.\n",
    "        \n",
    "    Retorno:\n",
    "        message (str): mensagem detalhando o sucesso ou falha da operação.\n",
    "    \"\"\"\n",
    "    \n",
    "    try: # Se tiver arquivos.\n",
    "        arquivo = dbutils.fs.ls(dir_bronze)         # Lista o arquivo se existir.\n",
    "        df_result = ps.read_parquet(dir_bronze)     # Carrega em df.\n",
    "        df_result = ps.concat([df_result, df_new])  # Concatena os novos dados no df consolidado.\n",
    "        \n",
    "        # Verificação de arquivos duplicados.\n",
    "        df_result = df_result.sort_values(by='data_load')\n",
    "        df_result = df_result.drop_duplicates(subset='key', keep='last')\n",
    "\n",
    "        df_result.to_parquet(dir_bronze)            # Exportar o df consolidade (Parquet).\n",
    "\n",
    "        message = 'Resultado carregado com sucesso!'\n",
    "\n",
    "    except Exception as e: # Se não tiver arquivos.\n",
    "        if 'java.io.FileNotFoundException' in str(e):\n",
    "            message = \"Arquivo não encontrado para load_data, primeiro processamento\"\n",
    "            df_new.to_parquet(dir_bronze)\n",
    "\n",
    "        else: \n",
    "            message = 'Erro na carga: ' + str(e)\n",
    "\n",
    "    # Remove os dados brutos que foram carregados.\n",
    "    dbutils.fs.rm(f'{dir_landing_zone}/dados_extraidos.csv', True)\n",
    "\n",
    "    return message\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b43aa21d-e578-4a11-bbf2-9416f8193d35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Função para Limpeza dos dados (Silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aba8fb08-858a-4971-b4ea-182938bf9613",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clear_data() -> bool:\n",
    "    \"\"\"\n",
    "    Função que limpa os dados consolidados.\n",
    "    \n",
    "    Parâmetros:\n",
    "        None\n",
    "    \n",
    "    Retorno:\n",
    "        bool (bool): valor de sucesso ou falha da execução da função.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ler o dado consolidado.\n",
    "        df = ps.read_parquet(dir_bronze)\n",
    "\n",
    "        # Remover linhas com valores nulos.\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Mudar tipo de dados da coluna publishedAt.\n",
    "        # Arrumar a data de publicação.\n",
    "        df['publishedAt'] = df['publishedAt'].apply(lambda x : x.split('T')[0])\n",
    "        df['publishedAt'] = ps.to_datetime(df['publishedAt'], format='%Y-%m-%d')\n",
    "\n",
    "        # Armazenar parquet limpo.\n",
    "        df.to_parquet(dir_silver)\n",
    "\n",
    "        print('Limpeza realizada com sucesso!')\n",
    "\n",
    "        return True\n",
    "    except:\n",
    "        print('Falha: não foi possível realizar a limpeza dos dados!')\n",
    "\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70f9717-b2ca-443a-82e5-a3f6b1dfc5cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Função para Transformação dos dados (Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebdfded2-1ad8-412c-ac4b-360bf092b4ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def agrupar_by_data(df : ps.DataFrame) -> ps.DataFrame:\n",
    "    # Criar novas colunas.\n",
    "    df['Year'] = df['publishedAt'].dt.year\n",
    "    df['Month'] = df['publishedAt'].dt.month\n",
    "    df['Day'] = df['publishedAt'].dt.day\n",
    "\n",
    "    # Agrupar -> Por ano, mês e dia.\n",
    "    df = df.groupby(['Year', 'Month', 'Day']).agg(Qtde_noticias = ('title', 'Count'))\n",
    "\n",
    "    # Reinicia o index e não exclui o anterior.\n",
    "    df = df.reset_index(drop=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "538d4a64-b7a2-4d61-a868-3036d5920de1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def agrupar_by_source(df : ps.DataFrame) -> ps.DataFrame:\n",
    "    # Agrupar -> Por source e author.\n",
    "    df = df.groupby(['source', 'author']).agg(Qtde_noticias = ('title', 'Count'))\n",
    "\n",
    "    # Reinicia o index e não exclui o anterior.\n",
    "    df = df.reset_index(drop=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40280121-c96f-46bf-8d6f-95cc5efb0c35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def agrupar_by_key(df : ps.DataFrame) -> ps.DataFrame:\n",
    "    # Criar novas colunas.\n",
    "    df['Year'] = df['publishedAt'].dt.year\n",
    "    df['Month'] = df['publishedAt'].dt.month\n",
    "    df['Day'] = df['publishedAt'].dt.day\n",
    "    #df['Words'] = df['description'].apply(lambda x: x.split())\n",
    "\n",
    "    # Agrupar -> Por ano, mês e dia.\n",
    "    df = df.groupby(['Year', 'Month', 'Day']).agg(Qtde_noticias = ('title', 'Count'))\n",
    "\n",
    "    # Reinicia o index e não exclui o anterior.\n",
    "    df = df.reset_index(drop=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68092027-0c36-4672-a511-5fde32d9faad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_data() -> None:\n",
    "    \n",
    "    # Ler o dado limpo.\n",
    "    df = ps.read_parquet(dir_silver)\n",
    "\n",
    "    # 4.1 - Quantidade de notícias por ano, mês e dia de publicação;\n",
    "    df_groupDate = agrupar_by_data(df)\n",
    "\n",
    "    # 4.2 - Quantidade de notícias por fonte e autor;\n",
    "    df_groupSource = agrupar_by_source(df)\n",
    "\n",
    "    # 4.3 - Quantidade de notícias por palavra-chave;\n",
    "    df_groupWordKey = agrupar_by_key(df)\n",
    "\n",
    "    # Armazena os resultados transformados.\n",
    "    df_groupDate.to_parquet(f'{dir_gold}/dado_transformado_by_data.parquet')\n",
    "    df_groupSource.to_parquet(f'{dir_gold}/dado_transformado_by_source.parquet')\n",
    "    df_groupWordKey.to_parquet(f'{dir_gold}/dado_transformado_by_key.parquet')\n",
    "\n",
    "    print('Transformação realizada com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddad01e2-06ce-453b-806f-63cf1a81b9e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Função ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b321be46-0344-472d-9021-21482568ded6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def elt(current_datetime : str):\n",
    "    \"\"\"\n",
    "    Função que executa o processo de ELT.\n",
    "\n",
    "    Parâmetros:\n",
    "        current_datetime (str) : data para importar os dados.\n",
    "\n",
    "    Retorno:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"** inicializa o ELT **\\n\")\n",
    "\n",
    "    try: # Verifica se existe novos dados para serem processados.\n",
    "\n",
    "        # Faz a chamada da extração, da carga e da transformação dos dados.\n",
    "        df_new = extract_data(current_datetime)\n",
    "        \n",
    "        if df_new is not None: # Verifica se ocorreu a extração de dados corretamente.\n",
    "            message = load_data(df_new)\n",
    "            if message == 'Resultado carregado com sucesso!' or message == \"Arquivo não encontrado para load_data, primeiro processamento\": # Verifica se conseguiu carregar os dados.\n",
    "                print(message)\n",
    "                limpou = clear_data()\n",
    "                if limpou: # Verifica se conseguiu limpar os dados.\n",
    "                    transform_data()\n",
    "                    message = '\\n** ELT realizado com sucesso! **\\n'\n",
    "        else:\n",
    "            message = '\\n** ELT interrompido! **\\n'\n",
    "\n",
    "    except Exception as e: #caso não exista nenhum dado novo, retorna com a mensagem e encerra o processo\n",
    "            message = '\\nErro no ELT:' + str(e) + '\\n'\n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a4bba5-68ca-4546-9e82-f96d62d8a7eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Webhook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa261712-5a25-4019-95fc-0a2aaee119cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inicialização da aplicação Flask\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "# Definição da rota \"/webhook\" com suporte a requisições HTTP POST\n",
    "@app.route(\"/webhook\", methods=[\"POST\"])\n",
    "def handle_webhook():\n",
    "    # Recupera o conteúdo da requisição como um dicionário em Python\n",
    "    data = flask.request.get_json()\n",
    "    \n",
    "    # Imprime o conteúdo da requisição\n",
    "    print(\"Received data:\", data)\n",
    "    \n",
    "    # Data recebida.\n",
    "    current_datetime = data.get('current_datetime')\n",
    "\n",
    "    # Executar o elt.\n",
    "    message = elt(current_datetime)\n",
    "\n",
    "    # Mostra a mensagem!\n",
    "    print(message)\n",
    "\n",
    "    # Retorna uma resposta HTTP simples\n",
    "    return message\n",
    "\n",
    "# Verifica se o script está sendo executado como um módulo principal\n",
    "if __name__ == \"__main__\":\n",
    "    # Inicia a execução da aplicação\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Webhook Rodar",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
